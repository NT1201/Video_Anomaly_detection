
# Weakly-Supervised Video Anomaly Detection

This project implements a **weakly-supervised anomaly detection** framework for surveillance videos using a **Multiple Instance Learning (MIL)** strategy. It integrates features from **I3D** and **TimeSformer**, demonstrating high temporal precision and robustness on the **UCF-Crime** dataset.

## ðŸ“š Overview

Traditional video anomaly detection often relies on fully-supervised training or weakly expressive features. In contrast, this project fuses *spatial-temporal motion (I3D)* and *semantic transformer features (TimeSformer)*, and optimizes anomaly prediction using a **top-k MIL loss** under weak supervision (only video-level labels).

Key components:
- Dual feature extraction (I3D + TimeSformer).
- Segment-based MIL classifier with top-k loss.
- Confidence-based anomaly scoring and t-SNE/UMAP visualization.
- Evaluation via ROC, PR curves, and confusion matrix.

## ðŸ—‚ Project Structure

```
.
â”œâ”€â”€ main.py              # Training and evaluation loop
â”œâ”€â”€ dataset.py           # Dataset loader for feature-based videos
â”œâ”€â”€ learner.py           # Learner class with MIL classifier
â”œâ”€â”€ loss.py              # Top-k MIL loss function
â”œâ”€â”€ vis.py               # Visualization utilities (ROC, PR, confusion)
â”œâ”€â”€ tsne_vis.py          # t-SNE and UMAP embeddings visualization
â”œâ”€â”€ checkpoints/         # Trained models (optional)
â”œâ”€â”€ plots/               # Output visualizations (auto-created)
â””â”€â”€ data/
    â”œâ”€â”€ Training/
    â””â”€â”€ Testing/
```

## ðŸš€ Running the Code

### 1. Prerequisites

Install Python packages:

```bash
pip install torch torchvision matplotlib scikit-learn umap-learn plotly
```

### 2. Prepare Data

Extract features using I3D and TimeSformer and organize them:

```
data/
â”œâ”€â”€ Training/
â”‚   â”œâ”€â”€ Normal_I3D_RGB_NPY/
â”‚   â””â”€â”€ Anomaly_I3D_RGB_NPY/
â”œâ”€â”€ Testing/
â”‚   â”œâ”€â”€ Normal_I3D_RGB_NPY/
â”‚   â””â”€â”€ Anomaly_I3D_RGB_NPY/
```

Same for TimeSformer features in parallel folders.

### 3. Train the Model

```bash
python main.py --data_root path/to/data --feat both --batch 16 --seg 64
```

Model checkpoints are saved in `checkpoints/`.

### 4. Visualize Embeddings (t-SNE)

```bash
python tsne_vis.py --data_root path/to/data --feat_type both --checkpoint checkpoints/model.pth
```

Output images and interactive plots will be saved in `plots/`.

## ðŸ“ˆ Evaluation & Visualization

- **ROC / PR curves** and **confusion matrix** are saved per epoch under `plots/`.
- The following example shows segment-level anomaly scores across time, overlaid with keyframe thumbnails and ground-truth regions.

## ðŸ§  Key Results

| Method (Weak Supervision)                     | Backbone / Key Idea           | UCF-Crime AUC (%) |
|----------------------------------------------|-------------------------------|-------------------|
| **Ours: I3D + TimeSformer, Top-k MIL**        | Feature fusion + Top-k MIL    | **90.70**         |
| MSTAgent-VAD (VideoSwin, multi-scale, RTFM)   | Transformer + RTFM            | 89.27             |
| MSTAgent-VAD (I3D variant)                    | I3D + RTFM                     | 85.52             |
| Chen et al. 2023 (VST-RGB)                    | Vision Swin-Transformer       | 86.67             |
| Tian et al. 2021                              | I3D + smoothness, sparsity    | 84.30             |
| Sultani et al. 2018 (original MIL ranking)    | I3D                           | 75.41             |



## ðŸ›  Acknowledgements

- UCF-Crime Dataset: https://www.kaggle.com/datasets/odins0n/ucf-crime-dataset
- MIL Loss based on: Sultani et al., 2018
- Transformer features via TimeSformer
